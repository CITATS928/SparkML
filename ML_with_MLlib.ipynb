{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7f1d6ab-3753-4417-8766-46b5755d9126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebfef562-489f-4dbe-ac75-3bffd461bdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/20 08:17:26 WARN Utils: Your hostname, Zipcoders-MacBook-Pro-2.local resolves to a loopback address: 127.0.0.1; using 192.168.3.169 instead (on interface en0)\n",
      "24/08/20 08:17:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/20 08:17:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#init SparkSession\n",
    "spark = SparkSession.builder.appName(\"Airbnb analysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4c2e756-2331-432e-b07e-2064b9a82716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "|neighbourhood_cleansed|      room_type|bedrooms|bathrooms|number_of_reviews|price|\n",
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "|      Western Addition|Entire home/apt|     1.0|      1.0|            180.0|170.0|\n",
      "|        Bernal Heights|Entire home/apt|     2.0|      1.0|            111.0|235.0|\n",
      "|        Haight Ashbury|   Private room|     1.0|      4.0|             17.0| 65.0|\n",
      "|        Haight Ashbury|   Private room|     1.0|      4.0|              8.0| 65.0|\n",
      "|      Western Addition|Entire home/apt|     2.0|      1.5|             27.0|785.0|\n",
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filepath = \"/Users/qian/Desktop/Zipcode/Python/LearningSparkV2/databricks-datasets/learning-spark-v2/sf-airbnb/sf-airbnb-clean.parquet/\"\n",
    "airbnbDF = spark.read.parquet(filepath)\n",
    "airbnbDF.select(\"neighbourhood_cleansed\", \"room_type\", \"bedrooms\", \"bathrooms\",\"number_of_reviews\", \"price\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ac0b562-0f77-4deb-bb9c-2f86d0b2916f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/20 08:17:32 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5780 rows in the training set, and 1366 in the test set\n"
     ]
    }
   ],
   "source": [
    "#Creat Train and test dataset\n",
    "#Ultimate Question of Life - 42\n",
    "trainDF, testDF= airbnbDF.randomSplit([0.8,0.2],seed = 42)\n",
    "print(f\"There are {trainDF.count()} rows in the training set, and {testDF.count()} in the test set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "546ffc4f-7579-4678-90fa-da5ec612ac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ade207fc-0917-46a9-a2a1-332eb899157f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-----+\n",
      "|bedrooms|features|price|\n",
      "+--------+--------+-----+\n",
      "|     1.0|   [1.0]|200.0|\n",
      "|     1.0|   [1.0]|130.0|\n",
      "|     1.0|   [1.0]| 95.0|\n",
      "|     1.0|   [1.0]|250.0|\n",
      "|     3.0|   [3.0]|250.0|\n",
      "|     1.0|   [1.0]|115.0|\n",
      "|     1.0|   [1.0]|105.0|\n",
      "|     1.0|   [1.0]| 86.0|\n",
      "|     1.0|   [1.0]|100.0|\n",
      "|     2.0|   [2.0]|220.0|\n",
      "+--------+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transform, return a new DF\n",
    "vecAssembler = VectorAssembler(inputCols=['bedrooms'], outputCol = 'features')\n",
    "vecTrainDF = vecAssembler.transform(trainDF)\n",
    "vecTrainDF.select('bedrooms','features','price').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1d22db9-7bbb-4187-9565-71a3db84d5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ee26df5-1c67-465e-a82f-88219b9e367e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/20 08:17:37 WARN Instrumentation: [3b28c95d] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/08/20 08:17:37 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/08/20 08:17:37 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "24/08/20 08:17:37 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(featuresCol='features',labelCol='price')\n",
    "lrModel = lr.fit(vecTrainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86e81590-ef4b-4dea-af3e-3520e3662b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The formula for the linear regression line is price = 123.68*bedrooms + 47.51\n"
     ]
    }
   ],
   "source": [
    "m = round (lrModel.coefficients[0],2)\n",
    "b = round (lrModel.intercept,2)\n",
    "print(f\"The formula for the linear regression line is price = {m}*bedrooms + {b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d729435b-6267-4d17-99a5-6eb0725879aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/20 08:17:39 WARN Instrumentation: [c2e7f981] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/08/20 08:17:39 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "# creating pipeline, using Pipeline API\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[vecAssembler,lr])\n",
    "pipelineModel = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b2a4a65-1338-4fe0-9ef4-56b48f491dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+------------------+\n",
      "|bedrooms|features| price|        prediction|\n",
      "+--------+--------+------+------------------+\n",
      "|     1.0|   [1.0]|  85.0|171.18598011578285|\n",
      "|     1.0|   [1.0]|  45.0|171.18598011578285|\n",
      "|     1.0|   [1.0]|  70.0|171.18598011578285|\n",
      "|     1.0|   [1.0]| 128.0|171.18598011578285|\n",
      "|     1.0|   [1.0]| 159.0|171.18598011578285|\n",
      "|     2.0|   [2.0]| 250.0|294.86172649777757|\n",
      "|     1.0|   [1.0]|  99.0|171.18598011578285|\n",
      "|     1.0|   [1.0]|  95.0|171.18598011578285|\n",
      "|     1.0|   [1.0]| 100.0|171.18598011578285|\n",
      "|     1.0|   [1.0]|2010.0|171.18598011578285|\n",
      "+--------+--------+------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predDF = pipelineModel.transform(testDF)\n",
    "predDF.select('bedrooms','features','price','prediction').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75f4f977-d4c1-4ad5-9a75-3f4fcda85873",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6833badb-488a-4c8a-9775-99e591487e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricalCols = [field for (field, dataType) in trainDF.dtypes if dataType == 'string']\n",
    "indexOutputCols = [x + 'Index' for x in categoricalCols]\n",
    "oheOutputCols = [x + 'OHE' for x in categoricalCols]\n",
    "\n",
    "stringIndexer = StringIndexer(inputCols=categoricalCols, outputCols=indexOutputCols, handleInvalid='skip')\n",
    "oheEncoder = OneHotEncoder(inputCols=indexOutputCols, outputCols=indexOutputCols)\n",
    "\n",
    "numericCols = [field for (field, dataType) in trainDF.dtypes if ((dataType=='double') & (field != 'price'))]\n",
    "assemblerInputs = oheOutputCols+numericCols\n",
    "vecAssembler = VectorAssembler(inputCols=assemblerInputs,outputCol = 'features')\n",
    "                                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3927e0c-8bc0-4226-a7d6-3b73bf4b15a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "rFormula = RFormula(formula = 'price ~ .',featuresCol='features',labelCol='price',handleInvalid='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9244d624-b53f-4f4d-b0b3-1585f8c308d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/20 08:17:44 WARN Instrumentation: [8684e943] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/08/20 08:17:44 WARN Instrumentation: [8684e943] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------------+\n",
      "|            features|price|        prediction|\n",
      "+--------------------+-----+------------------+\n",
      "|(98,[0,3,6,7,23,4...| 85.0| 55.30094763354373|\n",
      "|(98,[0,3,6,7,23,4...| 45.0| 22.70940291742818|\n",
      "|(98,[0,3,6,7,23,4...| 70.0|27.182906571761578|\n",
      "|(98,[0,3,6,7,13,4...|128.0|-91.90969569190747|\n",
      "|(98,[0,3,6,7,13,4...|159.0| 94.54162775821169|\n",
      "+--------------------+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(labelCol=\"price\", featuresCol=\"features\")\n",
    "#pipeline = Pipeline(stages = [stringIndexer, oheEncoder, vecAssembler, lr])\n",
    "\n",
    "# Or use RFormula\n",
    "pipeline = Pipeline(stages = [rFormula, lr])\n",
    "pipelineModel = pipeline.fit(trainDF)\n",
    "predDF = pipelineModel.transform(testDF)\n",
    "predDF.select(\"features\", \"price\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb1f67db-4719-4c1c-a5d3-d16a7f067d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE is 220.6\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "regressionEvaluator = RegressionEvaluator(\n",
    "predictionCol=\"prediction\",\n",
    "labelCol=\"price\",\n",
    "metricName=\"rmse\")\n",
    "rmse = regressionEvaluator.evaluate(predDF)\n",
    "print(f\"RMSE is {rmse:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "019aba97-66d8-4c4c-b66b-4726680d44e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 is 0.15985154393435386\n"
     ]
    }
   ],
   "source": [
    "r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(predDF)\n",
    "print(f\"R2 is {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f7c74ff-183e-48eb-9337-e9736fc010df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the Models\n",
    "pipelinePath = '/tmp/lr-pipeline-model'\n",
    "pipelineModel.write().overwrite().save(pipelinePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fef9dc5-f2fe-4585-9686-ea43d01b4593",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Loading the Models\n",
    "from pyspark.ml import PipelineModel\n",
    "savedPipelineModel = PipelineModel.load(pipelinePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e2074f4-44dc-47c0-9c92-d346c973cc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/20 08:17:51 ERROR Instrumentation: java.lang.IllegalArgumentException: requirement failed: DecisionTree requires maxBins (= 32) to be at least as large as the number of values in each categorical feature, but categorical feature 3 has 36 values. Consider removing this and other categorical features with a large number of values, or add more training examples.\n",
      "\tat scala.Predef$.require(Predef.scala:281)\n",
      "\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:151)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:274)\n",
      "\tat org.apache.spark.ml.regression.DecisionTreeRegressor.$anonfun$train$1(DecisionTreeRegressor.scala:135)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:116)\n",
      "\tat org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:46)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: DecisionTree requires maxBins (= 32) to be at least as large as the number of values in each categorical feature, but categorical feature 3 has 36 values. Consider removing this and other categorical features with a large number of values, or add more training examples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m stages \u001b[38;5;241m=\u001b[39m [stringIndexer, vecAssembler, dt]\n\u001b[1;32m     16\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline(stages \u001b[38;5;241m=\u001b[39m stages)\n\u001b[0;32m---> 17\u001b[0m pipelineModel \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainDF\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ZipCode/Python/myenv/lib/python3.12/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/ZipCode/Python/myenv/lib/python3.12/site-packages/pyspark/ml/pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    132\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mtransform(dataset)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[0;32m~/Desktop/ZipCode/Python/myenv/lib/python3.12/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/ZipCode/Python/myenv/lib/python3.12/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m~/Desktop/ZipCode/Python/myenv/lib/python3.12/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ZipCode/Python/myenv/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Desktop/ZipCode/Python/myenv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: DecisionTree requires maxBins (= 32) to be at least as large as the number of values in each categorical feature, but categorical feature 3 has 36 values. Consider removing this and other categorical features with a large number of values, or add more training examples."
     ]
    }
   ],
   "source": [
    "# Tree-Based Models\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "dt = DecisionTreeRegressor(labelCol='price')\n",
    "\n",
    "# Filter for just numeric columns (and exclude price, our label)\n",
    "numericCols = [field for (field, dataType) in trainDF.dtypes if ((dataType == \"double\") & (field != \"price\"))]\n",
    "\n",
    "# Combine output of StringIndexer defined above and numeric columns\n",
    "assemblerInputs = indexOutputCols + numericCols \n",
    "vecAssembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "\n",
    "# Combine stages into pipeline\n",
    "stages = [stringIndexer, vecAssembler, dt]\n",
    "\n",
    "pipeline = Pipeline(stages = stages)\n",
    "pipelineModel = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "993dcdb2-d3bd-4b4e-9228-d0125759e087",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.setMaxBins(40)\n",
    "pipelineModel = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a9571e8-7f8c-4bb0-8875-b0450f89e679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeRegressionModel: uid=DecisionTreeRegressor_aa29a145f806, depth=5, numNodes=47, numFeatures=33\n",
      "  If (feature 12 <= 2.5)\n",
      "   If (feature 12 <= 1.5)\n",
      "    If (feature 5 in {1.0,2.0})\n",
      "     If (feature 4 in {0.0,1.0,3.0,5.0,9.0,10.0,11.0,13.0,14.0,16.0,18.0,24.0})\n",
      "      If (feature 3 in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0})\n",
      "       Predict: 104.23992784125075\n",
      "      Else (feature 3 not in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0})\n",
      "       Predict: 250.7111111111111\n",
      "     Else (feature 4 not in {0.0,1.0,3.0,5.0,9.0,10.0,11.0,13.0,14.0,16.0,18.0,24.0})\n",
      "      If (feature 3 in {0.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,27.0,33.0,35.0})\n",
      "       Predict: 151.94179894179894\n",
      "      Else (feature 3 not in {0.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,27.0,33.0,35.0})\n",
      "       Predict: 245.8507462686567\n",
      "    Else (feature 5 not in {1.0,2.0})\n",
      "     If (feature 3 in {1.0,5.0,6.0,7.0,8.0,9.0,11.0,13.0,15.0,16.0,17.0,19.0,22.0,23.0,24.0,25.0,26.0,28.0,29.0,30.0,33.0})\n",
      "      If (feature 3 in {5.0,8.0,13.0,15.0,16.0,19.0,22.0,23.0,24.0,25.0,28.0,30.0,33.0})\n",
      "       Predict: 131.96658097686375\n",
      "      Else (feature 3 not in {5.0,8.0,13.0,15.0,16.0,19.0,22.0,23.0,24.0,25.0,28.0,30.0,33.0})\n",
      "       Predict: 164.19959266802445\n",
      "     Else (feature 3 not in {1.0,5.0,6.0,7.0,8.0,9.0,11.0,13.0,15.0,16.0,17.0,19.0,22.0,23.0,24.0,25.0,26.0,28.0,29.0,30.0,33.0})\n",
      "      If (feature 10 <= 6.5)\n",
      "       Predict: 205.5814889336016\n",
      "      Else (feature 10 > 6.5)\n",
      "       Predict: 841.6666666666666\n",
      "   Else (feature 12 > 1.5)\n",
      "    If (feature 13 <= 4.5)\n",
      "     If (feature 3 in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,15.0,16.0,17.0,18.0,19.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,33.0,34.0})\n",
      "      If (feature 14 <= 26.5)\n",
      "       Predict: 290.8357933579336\n",
      "      Else (feature 14 > 26.5)\n",
      "       Predict: 214.04819277108433\n",
      "     Else (feature 3 not in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,15.0,16.0,17.0,18.0,19.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,33.0,34.0})\n",
      "      If (feature 14 <= 3.5)\n",
      "       Predict: 741.64\n",
      "      Else (feature 14 > 3.5)\n",
      "       Predict: 309.03921568627453\n",
      "    Else (feature 13 > 4.5)\n",
      "     If (feature 15 <= 0.5)\n",
      "      If (feature 2 in {1.0})\n",
      "       Predict: 300.0\n",
      "      Else (feature 2 not in {1.0})\n",
      "       Predict: 10000.0\n",
      "     Else (feature 15 > 0.5)\n",
      "      If (feature 3 in {1.0,4.0,5.0,7.0,8.0,19.0})\n",
      "       Predict: 222.91666666666666\n",
      "      Else (feature 3 not in {1.0,4.0,5.0,7.0,8.0,19.0})\n",
      "       Predict: 398.0\n",
      "  Else (feature 12 > 2.5)\n",
      "   If (feature 1 in {0.0,1.0,2.0,3.0,4.0})\n",
      "    If (feature 12 <= 5.5)\n",
      "     If (feature 3 in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,10.0,11.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,21.0,22.0,23.0,24.0,25.0,26.0,28.0,29.0,30.0,33.0})\n",
      "      If (feature 14 <= 7.5)\n",
      "       Predict: 493.3795620437956\n",
      "      Else (feature 14 > 7.5)\n",
      "       Predict: 296.76666666666665\n",
      "     Else (feature 3 not in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,10.0,11.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,21.0,22.0,23.0,24.0,25.0,26.0,28.0,29.0,30.0,33.0})\n",
      "      If (feature 9 <= -122.411075)\n",
      "       Predict: 722.96875\n",
      "      Else (feature 9 > -122.411075)\n",
      "       Predict: 2399.4\n",
      "    Else (feature 12 > 5.5)\n",
      "     If (feature 4 in {0.0,1.0,5.0,7.0})\n",
      "      If (feature 3 in {0.0,3.0,6.0,25.0})\n",
      "       Predict: 609.5\n",
      "      Else (feature 3 not in {0.0,3.0,6.0,25.0})\n",
      "       Predict: 1715.0\n",
      "     Else (feature 4 not in {0.0,1.0,5.0,7.0})\n",
      "      Predict: 8000.0\n",
      "   Else (feature 1 not in {0.0,1.0,2.0,3.0,4.0})\n",
      "    Predict: 8000.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtModel = pipelineModel.stages[-1]\n",
    "print(dtModel.toDebugString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6834d2d-8fe5-4f1a-91e4-b92d16a696f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bedrooms</td>\n",
       "      <td>0.283406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cancellation_policyIndex</td>\n",
       "      <td>0.167893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>instant_bookableIndex</td>\n",
       "      <td>0.140081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>property_typeIndex</td>\n",
       "      <td>0.128179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>number_of_reviews</td>\n",
       "      <td>0.126233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neighbourhood_cleansedIndex</td>\n",
       "      <td>0.056200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>longitude</td>\n",
       "      <td>0.038810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>minimum_nights</td>\n",
       "      <td>0.029473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>beds</td>\n",
       "      <td>0.015218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>room_typeIndex</td>\n",
       "      <td>0.010905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>accommodates</td>\n",
       "      <td>0.003603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>host_is_superhostIndex</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>bathrooms_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>beds_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>review_scores_rating_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>review_scores_accuracy_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>review_scores_cleanliness_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>review_scores_value</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>review_scores_checkin_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>review_scores_communication_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>review_scores_location_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>bedrooms_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>review_scores_rating</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>review_scores_location</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>review_scores_communication</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>review_scores_checkin</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>review_scores_cleanliness</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>review_scores_accuracy</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bathrooms</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>latitude</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>host_total_listings_count</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bed_typeIndex</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>review_scores_value_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           feature  importance\n",
       "12                        bedrooms    0.283406\n",
       "1         cancellation_policyIndex    0.167893\n",
       "2            instant_bookableIndex    0.140081\n",
       "4               property_typeIndex    0.128179\n",
       "15               number_of_reviews    0.126233\n",
       "3      neighbourhood_cleansedIndex    0.056200\n",
       "9                        longitude    0.038810\n",
       "14                  minimum_nights    0.029473\n",
       "13                            beds    0.015218\n",
       "5                   room_typeIndex    0.010905\n",
       "10                    accommodates    0.003603\n",
       "0           host_is_superhostIndex    0.000000\n",
       "24                    bathrooms_na    0.000000\n",
       "25                         beds_na    0.000000\n",
       "26         review_scores_rating_na    0.000000\n",
       "27       review_scores_accuracy_na    0.000000\n",
       "28    review_scores_cleanliness_na    0.000000\n",
       "22             review_scores_value    0.000000\n",
       "29        review_scores_checkin_na    0.000000\n",
       "30  review_scores_communication_na    0.000000\n",
       "31       review_scores_location_na    0.000000\n",
       "23                     bedrooms_na    0.000000\n",
       "16            review_scores_rating    0.000000\n",
       "21          review_scores_location    0.000000\n",
       "20     review_scores_communication    0.000000\n",
       "19           review_scores_checkin    0.000000\n",
       "18       review_scores_cleanliness    0.000000\n",
       "17          review_scores_accuracy    0.000000\n",
       "11                       bathrooms    0.000000\n",
       "8                         latitude    0.000000\n",
       "7        host_total_listings_count    0.000000\n",
       "6                    bed_typeIndex    0.000000\n",
       "32          review_scores_value_na    0.000000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureImp =pd.DataFrame(list(zip(vecAssembler.getInputCols(), dtModel.featureImportances)),\n",
    "                         columns=[\"feature\", \"importance\"])\n",
    "featureImp.sort_values(by=\"importance\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9cb395d9-d34e-4570-a060-239acf15a011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forests\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "rf = RandomForestRegressor(labelCol=\"price\", maxBins=40, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d310406-bbf5-4048-948f-e114814df845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold Cross-Validation\n",
    "pipeline = Pipeline(stages = [stringIndexer, vecAssembler, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5af69fdb-22bc-4a96-a73e-7c70dade92e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "paramGrid = (ParamGridBuilder().addGrid(rf.maxDepth, [2, 4, 6]).addGrid(rf.numTrees, [10, 100]).build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "80496364-4978-4eb9-8c6e-c69bfa2ae279",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "03439734-e46a-4875-afb8-bcb1b1ec59bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/20 09:39:22 WARN DAGScheduler: Broadcasting large task binary with size 1324.3 KiB\n",
      "24/08/20 09:39:27 WARN DAGScheduler: Broadcasting large task binary with size 1157.9 KiB\n",
      "24/08/20 09:39:33 WARN DAGScheduler: Broadcasting large task binary with size 1198.7 KiB\n",
      "24/08/20 09:39:35 WARN DAGScheduler: Broadcasting large task binary with size 1224.9 KiB\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import CrossValidator\n",
    "cv = CrossValidator(estimator=pipeline, evaluator=evaluator, estimatorParamMaps=paramGrid, numFolds=3, seed=42)\n",
    "cvModel = cv.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "62ec75dd-1a80-4b1e-afc1-d29058559f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({Param(parent='RandomForestRegressor_4b103387ada4', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 2,\n",
       "   Param(parent='RandomForestRegressor_4b103387ada4', name='numTrees', doc='Number of trees to train (>= 1).'): 10},\n",
       "  291.1822640924783),\n",
       " ({Param(parent='RandomForestRegressor_4b103387ada4', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 2,\n",
       "   Param(parent='RandomForestRegressor_4b103387ada4', name='numTrees', doc='Number of trees to train (>= 1).'): 100},\n",
       "  286.7714750274078),\n",
       " ({Param(parent='RandomForestRegressor_4b103387ada4', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 4,\n",
       "   Param(parent='RandomForestRegressor_4b103387ada4', name='numTrees', doc='Number of trees to train (>= 1).'): 10},\n",
       "  287.6963245160818),\n",
       " ({Param(parent='RandomForestRegressor_4b103387ada4', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 4,\n",
       "   Param(parent='RandomForestRegressor_4b103387ada4', name='numTrees', doc='Number of trees to train (>= 1).'): 100},\n",
       "  279.9927057236079),\n",
       " ({Param(parent='RandomForestRegressor_4b103387ada4', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 6,\n",
       "   Param(parent='RandomForestRegressor_4b103387ada4', name='numTrees', doc='Number of trees to train (>= 1).'): 10},\n",
       "  294.34810870889305),\n",
       " ({Param(parent='RandomForestRegressor_4b103387ada4', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 6,\n",
       "   Param(parent='RandomForestRegressor_4b103387ada4', name='numTrees', doc='Number of trees to train (>= 1).'): 100},\n",
       "  275.39862704729984)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(cvModel.getEstimatorParamMaps(), cvModel.avgMetrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa8aa95f-58c2-491a-a2f0-19641a529a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/20 09:45:27 WARN BlockManager: Block rdd_2253_0 already exists on this machine; not re-adding it\n",
      "24/08/20 09:45:29 WARN DAGScheduler: Broadcasting large task binary with size 1324.3 KiB\n",
      "24/08/20 09:45:32 WARN DAGScheduler: Broadcasting large task binary with size 1157.9 KiB\n",
      "24/08/20 09:45:34 WARN DAGScheduler: Broadcasting large task binary with size 1198.7 KiB\n",
      "24/08/20 09:45:37 WARN DAGScheduler: Broadcasting large task binary with size 1224.9 KiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CrossValidatorModel_a35e1cac217c"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel = cv.setParallelism(4).fit(trainDF)\n",
    "cvModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8f17d609-34f6-45b7-bb10-472b06aa44cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/20 09:46:25 WARN DAGScheduler: Broadcasting large task binary with size 1323.7 KiB\n",
      "24/08/20 09:46:28 WARN DAGScheduler: Broadcasting large task binary with size 1160.5 KiB\n",
      "24/08/20 09:46:30 WARN DAGScheduler: Broadcasting large task binary with size 1198.2 KiB\n",
      "24/08/20 09:46:32 WARN DAGScheduler: Broadcasting large task binary with size 1224.9 KiB\n"
     ]
    }
   ],
   "source": [
    "cv = CrossValidator(estimator=rf, evaluator=evaluator, estimatorParamMaps=paramGrid, numFolds=3, parallelism=4, seed=42)\n",
    "\n",
    "pipeline = Pipeline(stages=[stringIndexer, vecAssembler, cv])\n",
    "pipelineModel = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fab3097-a5e3-449c-beb7-74abe4bbefc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
